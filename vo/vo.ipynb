{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oltMOCo4gkD0"
   },
   "source": [
    "# Visual Odometry (VO)\n",
    "\n",
    "In this assignment, you do not need. a GPU. You will use the pykitti module and KITTI odometry dataset.\n",
    "\n",
    "You can download the odometry data from [here](https://drive.google.com/file/d/1PJOUnM3nEwDpqiRvfqUnkNPJZpM4PKYV/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffiRr-EEgkD9"
   },
   "source": [
    "## Monocular VO with OpenCV on KITTI\n",
    "\n",
    "For each consecutive frame pair in the sequence, you will compute the relative pose between the frames and visualize it. You will use:\n",
    "\n",
    "* pykitti code similar to what you wrote in mvs part to load the seqeunce with ground-truth info. (Check out the [demo code](https://github.com/utiasSTARS/pykitti/blob/master/demos/demo_odometry.py))\n",
    "* OpenCV functions to compute and visualize the features and the essential matrix.\n",
    "\n",
    "Please follow these steps to complete the assignment:\n",
    "\n",
    "1. You can use the ORB Feature to do the feature matching:\n",
    "    `orb = cv2.ORB_create()` to create the ORB object\n",
    "    and then `orb.detectAndCompute()` to find the keypoints and descriptors on both frames\n",
    "\n",
    "2. You can use brute-force matcher to match ORB descriptors:\n",
    "    `bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)`\n",
    "\n",
    "3. After matching the descriptors, sort the matched keypoints.\n",
    "\n",
    "4. Draw matches on the two images using the `cv2.drawMatches()` function.\n",
    "\n",
    "5. Compute the essential matrix using the `cv2.findEssentialMat()` function. Note that you need the matching points and the instrinsics for this function. \n",
    "\n",
    "6. Extract the rotation and translation from the essential matrix using the `cv2.recoverPose()` function.\n",
    "\n",
    "7. Multiply the estimated rotation and translation with the previous rotation and translation. Initialize rotation to identity and translation to zeros on the first frame.\n",
    "\n",
    "8. Display the current image with the keypoints on it using the `cv2.drawKeypoints()` function.\n",
    "\n",
    "9. Update the previous rotation and translation as the current rotation and translation.\n",
    "\n",
    "10. Draw the estimated trajectory as blue and ground-truth trajectory as green. You can use the `cv2.circle()` function.\n",
    "\n",
    "\n",
    "You can create a video of your visualization of images and poses for the provided sequence.\n",
    "\n",
    "**Bonus**: Compute the absolute trajectory error between the estimated trajectory and the ground-truth trajectory. \n",
    "\n",
    "Some examples repositories that might be useful:\n",
    "* https://bitbucket.org/castacks/visual_odometry_tutorial/src/master/visual-odometry/\n",
    "* https://github.com/uoip/monoVO-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute trajectory error: 4.606227611036428\n",
      "Creating videos\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "######## DISCLAIMER: I have borrowed most of the code from the given example repository: https://bitbucket.org/castacks/visual_odometry_tutorial/src/master/visual-odometry/\n",
    "# I have read and understood the given code, I just thought it would be meaningless to try and change the variable names etc., so most of the code is the same with this repo\n",
    "\n",
    "import os\n",
    "import imageio\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "import pykitti\n",
    "\n",
    "basedir = \"KITTI_odometry/\"\n",
    "sequence = \"09\"\n",
    "# Using the images from camera 2\n",
    "img_data_dir = \"KITTI_odometry/sequences/\" + sequence + '/image_2/'\n",
    "dataset = pykitti.odometry(basedir, sequence)\n",
    "\n",
    "# MOST OF THE CODE BELOW IS BORROWED FROM: https://bitbucket.org/castacks/visual_odometry_tutorial/src/master/visual-odometry/vo.py\n",
    "\n",
    "len_trajMap = 370 # I tried to set this to 1000, which was the default value in the given repository, but I encountered an error saying that this should be 370\n",
    "trajMap = np.zeros((len_trajMap, len_trajMap, 3), dtype=np.uint8)\n",
    "\n",
    "# get the camera 2 intrinsic parameters\n",
    "calibration = dataset.calib.K_cam2\n",
    "fx = calibration[0,0]\n",
    "fy = calibration[1,1]\n",
    "cx = calibration[0,2]\n",
    "cy = calibration[1,2]\n",
    "\n",
    "# image directory\n",
    "img_list = glob(img_data_dir + '/*.png')\n",
    "img_list.sort()\n",
    "num_frames = len(img_list)\n",
    "\n",
    "# these lists are for creating the videos\n",
    "matched_images = []\n",
    "keypoints = []\n",
    "trajectories = []\n",
    "\n",
    "# to keep the errors\n",
    "errors = []\n",
    "for i in range(num_frames):\n",
    "    curr_img = cv2.imread(img_list[i], 0)\n",
    "    if i == 0:\n",
    "        # initializing rotation to identity and translation to zeros for the first frame\n",
    "        curr_R = np.eye(3)\n",
    "        curr_t = np.array([0, 0, 0])\n",
    "    else:\n",
    "        prev_img = cv2.imread(img_list[i-1], 0)\n",
    "\n",
    "        # 1. orb features\n",
    "        orb = cv2.ORB_create()\n",
    "        kp1, des1 = orb.detectAndCompute(prev_img, None)\n",
    "        kp2, des2 = orb.detectAndCompute(curr_img, None)\n",
    "\n",
    "        # 2. match descriptors with the brute force matcher\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "\n",
    "        # 3. sorting the matches\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # 4. drawing the matches\n",
    "        img_matching = cv2.drawMatches(prev_img, kp1, curr_img, kp2, matches[0:100], None)\n",
    "        cv2.imwrite(f'matched_images/{i}.png', img_matching)\n",
    "        matched_images.append(imageio.imread(f'matched_images/{i}.png'))\n",
    "        \n",
    "        # 5. compute essential matrix\n",
    "        pts1 = np.float32([kp1[m.queryIdx].pt for m in matches])\n",
    "        pts2 = np.float32([kp2[m.trainIdx].pt for m in matches])\n",
    "        E, mask = cv2.findEssentialMat(pts1, pts2, focal=fx, pp=(cx, cy), method=cv2.RANSAC, prob=0.999, threshold=1)\n",
    "        pts1 = pts1[mask.ravel() == 1]\n",
    "        pts2 = pts2[mask.ravel() == 1]\n",
    "        \n",
    "        # 6. extract the rotation and the translation\n",
    "        _, R, t, mask = cv2.recoverPose(E, pts1, pts2, focal=fx, pp=(cx, cy))\n",
    "        \n",
    "        # 7. multiplying with the previous rotation and translation to obtain the camera motion\n",
    "        R = R.transpose()\n",
    "        t = -np.matmul(R, t)\n",
    "\n",
    "        if i == 1:\n",
    "            curr_R = R\n",
    "            curr_t = t\n",
    "        else:\n",
    "            curr_R = np.matmul(prev_R, R)\n",
    "            curr_t = np.matmul(prev_R, t) + prev_t\n",
    "            \n",
    "        # 8. draw the current image with keypoints\n",
    "        curr_img_kp = cv2.drawKeypoints(curr_img, kp2, None, color=(0, 255, 0), flags=0)\n",
    "        cv2.imwrite(f'keypoints/{i}.png', curr_img_kp)\n",
    "        keypoints.append(imageio.imread(f'keypoints/{i}.png'))\n",
    "        \n",
    "        # 9. update the previous rotation and translation\n",
    "        prev_R = curr_R\n",
    "        prev_t = curr_t\n",
    "        \n",
    "        # obtain the ground truth trajectory\n",
    "        gt = dataset.poses[i][0:3,-1]\n",
    "        \n",
    "        # calculate and save the absolute trajectory error (mean for all coordinates)\n",
    "        # to calculate the error, i simply take the difference between the predicted trajectory and the \n",
    "        # ground truth trajectory, take the absolute value and take mean across all the coordinates\n",
    "        errors.append(np.mean(np.abs(gt - curr_t.squeeze())))\n",
    "\n",
    "        # 10. drawing the estimated and gt trajectories\n",
    "        offset_draw = (int(len_trajMap/2))\n",
    "        cv2.circle(trajMap, (int(gt[0])+offset_draw, int(gt[2])+offset_draw), 1, (0,255,0), 2) # green for the ground truth\n",
    "        cv2.circle(trajMap, (int(curr_t[0])+offset_draw, int(curr_t[2])+offset_draw), 1, (255,0,0), 2) # blue for the estimation\n",
    "        \n",
    "        cv2.imwrite(f'trajectories/{i}.png', trajMap.astype(np.uint8))\n",
    "        trajectories.append(imageio.imread(f'trajectories/{i}.png'))\n",
    "\n",
    "# calculate the mean error\n",
    "absolute_trajectory_error = np.mean(errors)\n",
    "print(f'Mean absolute trajectory error: {absolute_trajectory_error}')\n",
    "\n",
    "# save the frames as a gif file\n",
    "print('Creating videos')\n",
    "imageio.mimsave(\"matched_images.gif\", matched_images)\n",
    "imageio.mimsave(\"keypoints.gif\", keypoints)\n",
    "imageio.mimsave(\"trajectories.gif\", trajectories)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "flownet3d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
